{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbfd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up dataset...\n",
      " train/pos: 12500 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshal\\.conda\\envs\\NLTK_env\\lib\\site-packages\\smart_open\\smart_open_lib.py:400: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "\n",
    "dirname = 'aclImdb'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "all_lines = []\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "if not os.path.isfile('aclImdb/alldata-id.txt'):\n",
    "    if not os.path.isdir(dirname):\n",
    "        if not os.path.isfile(filename):\n",
    "            # Download IMDB archive\n",
    "            print(\"Downloading IMDB archive...\")\n",
    "            url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "            r = requests.get(url)\n",
    "            with smart_open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "        tar = tarfile.open(filename, mode='r')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(\"IMDB archive directory already available without download.\")\n",
    "\n",
    "    # Collect & normalize test/train data\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "all_lines=[]\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "# Collect & normalize test/train data\n",
    "print(\"Cleaning up dataset...\")\n",
    "folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg', 'train/unsup']\n",
    "for fol in folders:\n",
    "    temp = u''\n",
    "    newline = \"\\n\".encode(\"utf-8\")\n",
    "    output = fol.replace('/', '-') + '.txt'\n",
    "    # Is there a better pattern to use?\n",
    "    txt_files = glob.glob(fol+'/*.txt')\n",
    "    print(\" %s: %i files\" % (fol, len(txt_files)))\n",
    "    with smart_open(output, \"wb\") as n:\n",
    "        for i, txt in enumerate(txt_files):\n",
    "            with smart_open(txt, \"rb\") as t:\n",
    "                one_text = t.read().decode(\"utf-8\")\n",
    "                for c in control_chars:\n",
    "                    one_text = one_text.replace(c, ' ')\n",
    "                one_text = normalize_text(one_text)\n",
    "                all_lines.append(one_text)\n",
    "                n.write(one_text.encode(\"utf-8\"))\n",
    "                n.write(newline)\n",
    "\n",
    "# Save to disk for instant re-use on any future runs\n",
    "with smart_open('alldata-id.txt', 'wb') as f:\n",
    "    for idx, line in enumerate(all_lines):\n",
    "        num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "        f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "print(\"Success, alldata-id.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d434698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshal\\.conda\\envs\\NLTK_env\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Darshal\\.conda\\envs\\NLTK_env\\lib\\site-packages\\smart_open\\smart_open_lib.py:400: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "from smart_open import smart_open\n",
    "\n",
    "# this data object class suffices as a `TaggedDocument` (with `words` and `tags`) \n",
    "# plus adds other state helpful for our later evaluation/reporting\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []\n",
    "with smart_open('alldata-id.txt', 'rb', encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # 'tags = [tokens[0]]' would also work at extra memory cost\n",
    "        split = ['train', 'test', 'extra', 'extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(alldocs), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43e801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "doc_list = alldocs[:]  \n",
    "shuffle(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75085b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,t8) vocabulary scanned & state initialized\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,t8) vocabulary scanned & state initialized\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t8) vocabulary scanned & state initialized\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=3, workers=cores),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=3, workers=cores, alpha=0.05, comment='alpha=0.05'),\n",
    "    # PV-DM w/ concatenation - big, slow, experimental mode\n",
    "    # window=5 (both sides) approximates paper's apparent 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=3, workers=cores),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(alldocs)\n",
    "    print(\"%s vocabulary scanned & state initialized\" % model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21a351eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    \"\"\"Fit a statsmodel logistic predictor on supplied data\"\"\"\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    # print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, \n",
    "                         reinfer_train=False, reinfer_test=False, \n",
    "                         infer_steps=None, infer_alpha=None, infer_subsample=0.2):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets = [doc.sentiment for doc in train_set]\n",
    "    if reinfer_train:\n",
    "        train_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in train_set]\n",
    "    else:\n",
    "        train_regressors = [test_model.docvecs[doc.tags[0]] for doc in train_set]\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if reinfer_test:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # Predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc7e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "error_rates = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e85b09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec(dbow,d100,n5,mc2,t8)\n",
      "\n",
      "Evaluating Doc2Vec(dbow,d100,n5,mc2,t8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-7664142d2416>:21: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  train_regressors = [test_model.docvecs[doc.tags[0]] for doc in train_set]\n",
      "<ipython-input-4-7664142d2416>:31: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.146240 Doc2Vec(dbow,d100,n5,mc2,t8)\n",
      "\n",
      "Training Doc2Vec(dm/m,d100,n5,w10,mc2,t8)\n",
      "\n",
      "Evaluating Doc2Vec(dm/m,d100,n5,w10,mc2,t8)\n",
      "\n",
      "0.211760 Doc2Vec(dm/m,d100,n5,w10,mc2,t8)\n",
      "\n",
      "Training Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n",
      "\n",
      "Evaluating Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n",
      "\n",
      "0.376800 Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in simple_models: \n",
    "    print(\"Training %s\" % model)\n",
    "    model.train(doc_list, total_examples=len(doc_list), epochs=model.epochs)\n",
    "    \n",
    "    print(\"\\nEvaluating %s\" % model)\n",
    "    err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs)\n",
    "    error_rates[str(model)] = err_rate\n",
    "    print(\"\\n%f %s\\n\" % (err_rate, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c0f9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=1\n",
    "for model in simple_models: \n",
    "    model.save(str(c))\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a7137c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Err_rate Model\n",
      "0.146240 Doc2Vec(dbow,d100,n5,mc2,t8)\n",
      "0.211760 Doc2Vec(dm/m,d100,n5,w10,mc2,t8)\n",
      "0.376800 Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Err_rate Model\")\n",
    "for rate, name in sorted((rate, name) for name, rate in error_rates.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03f87b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 20157...\n",
      "Doc2Vec(dbow,d100,n5,mc2,t8):\n",
      " [(20157, 0.940758228302002), (71138, 0.8800333142280579), (63987, 0.8795598149299622)] \n",
      " ['this', 'title', 'seems', 'more', 'like', 'a', 'filming', 'exercise', 'than', 'a', 'film', 'that', 'should', 'have', 'been', 'released', 'to', 'be', 'seen', 'by', 'the', 'public', '.', 'for', 'dafoe', 'and', 'his', 'wife', 'it', 'must', 'have', 'been', 'fun', 'working', 'together', 'in', 'a', 'film', 'for', 'the', 'first', 'time', ',', 'without', 'taking', 'into', 'consideration', 'that', 'people', 'might', 'actually', 'watch', 'it', '.', 'i', 'felt', 'like', 'it', 'was', '90mins', 'wasted', 'as', 'i', 'waited', 'anxiously', 'for', 'a', 'plot', 'to', 'develop', ',', 'or', 'even', 'begin', '.', 'try', 'to', 'fit', 'this', 'film', 'into', 'a', 'genre', 'and', 'you', \"won't\", ',', 'because', 'it', 'lacks', 'a', 'beginning', ',', 'middle', 'or', 'ending', '.', \"i've\", 'seen', \"'arty'\", 'movies', 'before', 'and', 'this', \"doesn't\", 'even', 'come', 'close', 'to', 'being', 'arty', ',', 'abstract', 'or', 'original', ',', 'it', 'just', 'seems', 'to', 'me', 'to', 'be', 'completely', 'pointless', '.', 'i', 'think', 'it', 'speaks', 'for', 'itself', 'when', 'the', 'only', 'persons', 'that', 'rated', 'this', 'film', 'a', '10', 'were', 'the', 'under', '18', 'age', 'group', '.', 'no', 'doubt', 'for', 'the', 'constant', 'pointless', 'erotic', 'scenes', 'that', 'the', 'film', 'was', 'insistent', 'on', 'throwing', 'at', 'us', '.', 'that', 'is', 'if', 'you', 'can', 'call', 'it', 'erotic', '.', 'it', 'certainly', \"didn't\", 'have', 'taste', '.']\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,t8):\n",
      " [(20157, 0.807192862033844), (92180, 0.7910299301147461), (6986, 0.790420413017273)] \n",
      " ['this', 'title', 'seems', 'more', 'like', 'a', 'filming', 'exercise', 'than', 'a', 'film', 'that', 'should', 'have', 'been', 'released', 'to', 'be', 'seen', 'by', 'the', 'public', '.', 'for', 'dafoe', 'and', 'his', 'wife', 'it', 'must', 'have', 'been', 'fun', 'working', 'together', 'in', 'a', 'film', 'for', 'the', 'first', 'time', ',', 'without', 'taking', 'into', 'consideration', 'that', 'people', 'might', 'actually', 'watch', 'it', '.', 'i', 'felt', 'like', 'it', 'was', '90mins', 'wasted', 'as', 'i', 'waited', 'anxiously', 'for', 'a', 'plot', 'to', 'develop', ',', 'or', 'even', 'begin', '.', 'try', 'to', 'fit', 'this', 'film', 'into', 'a', 'genre', 'and', 'you', \"won't\", ',', 'because', 'it', 'lacks', 'a', 'beginning', ',', 'middle', 'or', 'ending', '.', \"i've\", 'seen', \"'arty'\", 'movies', 'before', 'and', 'this', \"doesn't\", 'even', 'come', 'close', 'to', 'being', 'arty', ',', 'abstract', 'or', 'original', ',', 'it', 'just', 'seems', 'to', 'me', 'to', 'be', 'completely', 'pointless', '.', 'i', 'think', 'it', 'speaks', 'for', 'itself', 'when', 'the', 'only', 'persons', 'that', 'rated', 'this', 'film', 'a', '10', 'were', 'the', 'under', '18', 'age', 'group', '.', 'no', 'doubt', 'for', 'the', 'constant', 'pointless', 'erotic', 'scenes', 'that', 'the', 'film', 'was', 'insistent', 'on', 'throwing', 'at', 'us', '.', 'that', 'is', 'if', 'you', 'can', 'call', 'it', 'erotic', '.', 'it', 'certainly', \"didn't\", 'have', 'taste', '.']\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t8):\n",
      " [(55918, 0.5771116018295288), (27551, 0.5585404634475708), (4602, 0.5568709969520569)] \n",
      " ['this', 'title', 'seems', 'more', 'like', 'a', 'filming', 'exercise', 'than', 'a', 'film', 'that', 'should', 'have', 'been', 'released', 'to', 'be', 'seen', 'by', 'the', 'public', '.', 'for', 'dafoe', 'and', 'his', 'wife', 'it', 'must', 'have', 'been', 'fun', 'working', 'together', 'in', 'a', 'film', 'for', 'the', 'first', 'time', ',', 'without', 'taking', 'into', 'consideration', 'that', 'people', 'might', 'actually', 'watch', 'it', '.', 'i', 'felt', 'like', 'it', 'was', '90mins', 'wasted', 'as', 'i', 'waited', 'anxiously', 'for', 'a', 'plot', 'to', 'develop', ',', 'or', 'even', 'begin', '.', 'try', 'to', 'fit', 'this', 'film', 'into', 'a', 'genre', 'and', 'you', \"won't\", ',', 'because', 'it', 'lacks', 'a', 'beginning', ',', 'middle', 'or', 'ending', '.', \"i've\", 'seen', \"'arty'\", 'movies', 'before', 'and', 'this', \"doesn't\", 'even', 'come', 'close', 'to', 'being', 'arty', ',', 'abstract', 'or', 'original', ',', 'it', 'just', 'seems', 'to', 'me', 'to', 'be', 'completely', 'pointless', '.', 'i', 'think', 'it', 'speaks', 'for', 'itself', 'when', 'the', 'only', 'persons', 'that', 'rated', 'this', 'film', 'a', '10', 'were', 'the', 'under', '18', 'age', 'group', '.', 'no', 'doubt', 'for', 'the', 'constant', 'pointless', 'erotic', 'scenes', 'that', 'the', 'film', 'was', 'insistent', 'on', 'throwing', 'at', 'us', '.', 'that', 'is', 'if', 'you', 'can', 'call', 'it', 'erotic', '.', 'it', 'certainly', \"didn't\", 'have', 'taste', '.']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "doc_id = random.randint(0, len(test_docs))  # Pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s \\n %s' % (model, model.dv.most_similar([inferred_docvec], topn=3),alldocs[doc_id].words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f07d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'goodbye' (6157 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dbow,d100,n5,mc2,t8)</th><th>Doc2Vec(dm/m,d100,n5,w10,mc2,t8)</th><th>Doc2Vec(dm/c,d100,n5,w5,mc2,t8)</th></tr><tr><td>[('parrish', 0.46560177206993103),<br>\n",
       "('career-dead', 0.4251338839530945),<br>\n",
       "('scarp', 0.42053326964378357),<br>\n",
       "('ojibway', 0.39109230041503906),<br>\n",
       "('anti-competitive', 0.3903639018535614),<br>\n",
       "(\"shaffer's\", 0.3867054581642151),<br>\n",
       "('splashed', 0.386642724275589),<br>\n",
       "('rigors', 0.37911680340766907),<br>\n",
       "('on-point', 0.376544713973999),<br>\n",
       "(\"lessons'\", 0.3757546544075012),<br>\n",
       "('nick@night', 0.3698621690273285),<br>\n",
       "(\"'back\", 0.36863091588020325),<br>\n",
       "('benita', 0.3670118749141693),<br>\n",
       "('wildman', 0.3651280403137207),<br>\n",
       "('draco', 0.36431047320365906),<br>\n",
       "('football', 0.3607681393623352),<br>\n",
       "('gamely', 0.35533466935157776),<br>\n",
       "(\"dench's\", 0.35527360439300537),<br>\n",
       "('haryanvi', 0.3539436161518097),<br>\n",
       "(\"resume's\", 0.3528524339199066)]</td><td>[('farewell', 0.7002571225166321),<br>\n",
       "('good-bye', 0.5677174925804138),<br>\n",
       "('hello', 0.5641282796859741),<br>\n",
       "('adieu', 0.5598751306533813),<br>\n",
       "('beaver', 0.5391549468040466),<br>\n",
       "('stairway', 0.535651683807373),<br>\n",
       "('havana', 0.5307672619819641),<br>\n",
       "('needless', 0.5158049464225769),<br>\n",
       "('paraphrase', 0.5150619745254517),<br>\n",
       "('hi', 0.5129165053367615),<br>\n",
       "('inmate/pilot', 0.5123285055160522),<br>\n",
       "('trouby', 0.5072649717330933),<br>\n",
       "('hush', 0.5031053423881531),<br>\n",
       "('denver', 0.5025613903999329),<br>\n",
       "('hereafter', 0.5008232593536377),<br>\n",
       "('yi-che', 0.49571216106414795),<br>\n",
       "('nahi', 0.49128982424736023),<br>\n",
       "('reply', 0.4890081584453583),<br>\n",
       "('cuddle', 0.48873332142829895),<br>\n",
       "('tuesday', 0.4840763211250305)]</td><td>[('goodnight', 0.6841808557510376),<br>\n",
       "('hello', 0.6766211986541748),<br>\n",
       "('farewell', 0.5839443802833557),<br>\n",
       "(\"'penis'\", 0.5594010353088379),<br>\n",
       "('good-bye', 0.5414263010025024),<br>\n",
       "('bye', 0.5398421883583069),<br>\n",
       "('incoherently', 0.5338912010192871),<br>\n",
       "('afterward', 0.5278379321098328),<br>\n",
       "(\"'yes'\", 0.5205431580543518),<br>\n",
       "('whaaa', 0.5163118839263916),<br>\n",
       "('brana', 0.509772002696991),<br>\n",
       "('belonging', 0.5021269917488098),<br>\n",
       "('kuch', 0.49723172187805176),<br>\n",
       "(\"'bizarre\", 0.49608299136161804),<br>\n",
       "('afterwards', 0.493028461933136),<br>\n",
       "(\"'welcome\", 0.4914415776729584),<br>\n",
       "('hi', 0.4895091652870178),<br>\n",
       "('whoa', 0.48633822798728943),<br>\n",
       "('-couple', 0.4839484691619873),<br>\n",
       "('lonnrot', 0.48105746507644653)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_models = simple_models[:]\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index_to_key)\n",
    "    if word_models[0].wv.key_to_index[word] > 10:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "#word = 'comedy/drama'\n",
    "similars_per_model = [str(model.wv.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.key_to_index[word]))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0676f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b32fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
